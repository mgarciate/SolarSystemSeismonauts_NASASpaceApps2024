{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# SEISMIC DETECTION: TWO STEPS ENSEMBLE ALGORITHM\n",
        "This notebook describes and applies our chosen approach to solving the challenge.\n",
        "\n",
        "We opted to use the STA/LTA (Short-Term Average/Long-Term Average) algorithm with a grid of hyperparameters that minimize the absolute time error to obtain candidate seismic windows.\n",
        "\n",
        "From the detected candidate windows, we select the one with the highest number of anomalies, identified using an Isolation Forest.\n",
        "\n",
        "With this approach, we achieved a mean absolute error of nn seconds in a sample of 50 tested files."
      ],
      "metadata": {
        "id": "lS8bW7LwhwlI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install obspy"
      ],
      "metadata": {
        "collapsed": true,
        "id": "5ZgsqtfghySq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import obspy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from obspy import read\n",
        "from datetime import datetime, timedelta\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, Dict\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "from obspy.signal.invsim import cosine_taper\n",
        "from obspy.signal.filter import highpass\n",
        "from obspy.signal.trigger import classic_sta_lta, plot_trigger, trigger_onset\n",
        "from itertools import product\n",
        "from scipy import stats"
      ],
      "metadata": {
        "id": "A5qTuiRXh7Tv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_directory = '/content/drive/My Drive/NASA/data/lunar/training/catalogs/'\n",
        "cat_file = cat_directory + 'apollo12_catalog_GradeA_final.csv'\n",
        "cat = pd.read_csv(cat_file)"
      ],
      "metadata": {
        "id": "2z5RwgC-iD9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STA/LTA\n",
        "The following code processes seismic data files to detect and characterize seismic events using the STA/LTA (Short-Term Average over Long-Term Average) algorithm.\n",
        "\n",
        "It reads seismic traces, computes the characteristic function (CFT) to highlight potential events, and identifies triggers where the CFT exceeds specified thresholds. For each detected event, it extracts a range of statistical features, such as duration, peak values, and statistical moments to create a detailed DataFrame of event characteristics. This could be use as a potential features for future approaches.\n",
        "\n",
        "This procedure is applied to all files in the dataset, aggregating the results into a single DataFrame for the posterior proccess (Isolation forest)"
      ],
      "metadata": {
        "id": "Rgwh4WhuiLV9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cat_mars_directory = '/content/drive/My Drive/NASA/data/mars/training/catalogs/'\n",
        "cat_lunar_directory = '/content/drive/My Drive/NASA/data/lunar/training/catalogs/'\n",
        "cat_file_lunar = cat_lunar_directory + 'apollo12_catalog_GradeA_final.csv'\n",
        "cat_file_mars = cat_mars_directory + 'Mars_InSight_training_catalog_final.csv'\n",
        "cat = pd.read_csv(cat_file_lunar)\n",
        "#cat.head()\n",
        "np.random.seed(42)\n",
        "muestra_cat = cat.sample(n=50, random_state=42)\n",
        "muestra_cat.set_index('filename', inplace=True)\n",
        "restantes_cat = cat.loc[~cat.index.isin(muestra_cat.index)]\n",
        "#restantes_cat.set_index('filename', inplace=True)\n",
        "#print(muestra_cat[\"filename\"].unique())"
      ],
      "metadata": {
        "id": "UblUqzdZiMuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following commented code allows us to determine the best hyperparameters for the algorithm. The result is applied to the function in the next section, 'obtencion_ventanas_sta_lta'."
      ],
      "metadata": {
        "id": "CO_mdG3kHLWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def evaluate_sta_lta(tr_data, sta_len, lta_len, thr_on, thr_off, real_arrival, sampling_rate):\n",
        "#     \"\"\"\n",
        "#     Evaluates the STA/LTA algorithm for detecting seismic events based on given parameters.\n",
        "#     \"\"\"\n",
        "#     cft = classic_sta_lta(tr_data, int(sta_len * sampling_rate), int(lta_len * sampling_rate))\n",
        "#     on_off = trigger_onset(cft, thr_on, thr_off)\n",
        "\n",
        "#     if len(on_off) == 0:\n",
        "#         return float('inf'), 0\n",
        "#     elif len(on_off) > 1:\n",
        "#         return float('inf'), len(on_off)\n",
        "#     detected_time = on_off[0][0] / sampling_rate\n",
        "#     time_diff = abs(detected_time - real_arrival)\n",
        "\n",
        "#     return time_diff, 1\n",
        "\n",
        "# def optimize_sta_lta(archivos, ruta_entrenamiento, cat, param_grid):\n",
        "#     \"\"\"\n",
        "#     Optimizes the STA/LTA parameters by evaluating a grid of possible parameter values and selecting the best configuration.\n",
        "#     \"\"\"\n",
        "#     best_score = float('inf')\n",
        "#     best_params = {}\n",
        "\n",
        "#     for sta_len, lta_len, thr_on, thr_off in product(param_grid['sta_len'], param_grid['lta_len'],\n",
        "#                                                      param_grid['thr_on'], param_grid['thr_off']):\n",
        "#         total_diff = 0\n",
        "#         valid_files = 0\n",
        "#         multiple_detections = 0\n",
        "\n",
        "#         for nombre_archivo in archivos:\n",
        "#             try:\n",
        "#                 data = read(f\"{ruta_entrenamiento}{nombre_archivo}.mseed\")\n",
        "#                 tr = data.traces[0].copy()\n",
        "#                 tr_data = tr.data\n",
        "#                 sampling_rate = tr.stats.sampling_rate\n",
        "#                 starttime = tr.stats.starttime.datetime\n",
        "\n",
        "#                 fila_cat = cat[cat['filename'] == nombre_archivo].iloc[0]\n",
        "#                 arrival_time = datetime.strptime(fila_cat['time_abs(%Y-%m-%dT%H:%M:%S.%f)'], '%Y-%m-%dT%H:%M:%S.%f')\n",
        "#                 real_arrival = (arrival_time - starttime).total_seconds()\n",
        "\n",
        "#                 diff, num_detections = evaluate_sta_lta(tr_data, sta_len, lta_len, thr_on, thr_off, real_arrival, sampling_rate)\n",
        "\n",
        "#                 if num_detections == 1:\n",
        "#                     total_diff += diff\n",
        "#                     valid_files += 1\n",
        "#                 elif num_detections > 1:\n",
        "#                     multiple_detections += 1\n",
        "\n",
        "#             except Exception as e:\n",
        "#                 print(f\"Error procesando el archivo {nombre_archivo}: {str(e)}\")\n",
        "#                 continue\n",
        "\n",
        "\n",
        "#         if valid_files > 0:\n",
        "#             avg_diff = total_diff / valid_files\n",
        "#             detection_ratio = valid_files / len(archivos)\n",
        "#             score = avg_diff / detection_ratio\n",
        "\n",
        "#             if score < best_score:\n",
        "#                 best_score = score\n",
        "#                 best_params = {'sta_len': sta_len, 'lta_len': lta_len, 'thr_on': thr_on, 'thr_off': thr_off}\n",
        "\n",
        "#         print(f\"Parámetros: STA={sta_len}, LTA={lta_len}, ON={thr_on}, OFF={thr_off}\")\n",
        "#         print(f\"Archivos con detección única: {valid_files}/{len(archivos)}\")\n",
        "#         print(f\"Archivos con múltiples detecciones: {multiple_detections}\")\n",
        "#         print(f\"Score actual: {score if valid_files > 0 else float('inf')}\")\n",
        "#         print(\"-----\")\n",
        "\n",
        "#     return best_params, best_score\n",
        "\n",
        "# ruta_entrenamiento = './data/lunar/training/data/S12_GradeA/'\n",
        "# archivos = list(cat[\"filename\"].unique())[1:10]\n",
        "\n",
        "# param_grid = {\n",
        "#     'sta_len': [30, 60, 90, 120, 150, 180, 210],\n",
        "#     'lta_len': [300, 450, 600, 750, 900, 1050, 1200],\n",
        "#     'thr_on': [2.5, 3.0, 3.5, 4.0, 4.5, 5.0, 5.5, 6.0],\n",
        "#     'thr_off': [0.5, 0.75, 1.0, 1.25, 1.5, 1.75, 2.0]\n",
        "# }\n",
        "\n",
        "# best_params, best_score = optimize_sta_lta(archivos, ruta_entrenamiento, cat, param_grid)\n",
        "# print(f\"Mejores parámetros: {best_params}\")\n",
        "# print(f\"Mejor score: {best_score}\")"
      ],
      "metadata": {
        "id": "aaeXniCcGzvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_trigger_dataframe(cft, nombre_archivo, arrival, on_off, tr_times):\n",
        "    \"\"\"\n",
        "    Creates a DataFrame containing detailed trigger information for each seismic event detected.\n",
        "    \"\"\"\n",
        "    trigger_data = []\n",
        "    for i, (on, off) in enumerate(on_off):\n",
        "        cft_segment = cft[on:off+1]\n",
        "        trigger_data.append({\n",
        "            'Filename': nombre_archivo,\n",
        "            'Trigger': i + 1,\n",
        "            'On Time': tr_times[on],\n",
        "            'Off Time': tr_times[off],\n",
        "            'Arrival' : arrival,\n",
        "            'On Index': on,\n",
        "            'Off Index': off,\n",
        "            'Duration': tr_times[off] - tr_times[on],\n",
        "            'CFT On': cft[on],\n",
        "            'CFT Off': cft[off],\n",
        "            'CFT Max': np.max(cft_segment),\n",
        "            'CFT Min': np.min(cft_segment),\n",
        "            'CFT Mean': np.mean(cft_segment),\n",
        "            'CFT Median': np.median(cft_segment),\n",
        "            'CFT Std': np.std(cft_segment),\n",
        "            'CFT Skewness': stats.skew(cft_segment),\n",
        "            'CFT Kurtosis': stats.kurtosis(cft_segment),\n",
        "            'CFT Q1': np.percentile(cft_segment, 25),\n",
        "            'CFT Q3': np.percentile(cft_segment, 75),\n",
        "            'CFT IQR': np.percentile(cft_segment, 75) - np.percentile(cft_segment, 25),\n",
        "            'CFT Range': np.max(cft_segment) - np.min(cft_segment),\n",
        "            'CFT Coeff of Variation': np.std(cft_segment) / np.mean(cft_segment) if np.mean(cft_segment) != 0 else np.nan,\n",
        "            'CFT Peak to Mean Ratio': np.max(cft_segment) / np.mean(cft_segment) if np.mean(cft_segment) != 0 else np.nan,\n",
        "            'CFT Area Under Curve': np.trapz(cft_segment, tr_times[on:off+1]),\n",
        "            'CFT Max Index': on + np.argmax(cft_segment),\n",
        "            'CFT Max Time': tr_times[on + np.argmax(cft_segment)],\n",
        "            'CFT Slope': np.polyfit(tr_times[on:off+1], cft_segment, 1)[0],\n",
        "            'Arrival' : arrival\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(trigger_data)\n",
        "    return df\n",
        "\n",
        "def obtencion_ventanas_sta_lta(nombre_archivo, planeta):\n",
        "    \"\"\"\n",
        "    Extracts trigger windows from a seismic file using the STA/LTA algorithm based on the specified planet.\n",
        "    \"\"\"\n",
        "    if planeta.lower() == \"luna\":\n",
        "        ruta_entrenamiento = '/content/drive/My Drive/NASA/data/lunar/training/data/S12_GradeA/'\n",
        "    elif planeta.lower() == \"marte\":\n",
        "        ruta_entrenamiento = '/content/drive/My Drive/NASA/data/mars/training/data/'\n",
        "    else:\n",
        "        raise ValueError(\"El planeta debe ser 'luna' o 'marte'\")\n",
        "\n",
        "    data = read(f\"{ruta_entrenamiento}{nombre_archivo}.mseed\")\n",
        "    tr = data.traces[0].copy()\n",
        "    tr_times = tr.times()\n",
        "    tr_data = tr.data\n",
        "    fila_cat = cat[cat['filename'] == nombre_archivo].iloc[0]\n",
        "    arrival_time = datetime.strptime(fila_cat['time_abs(%Y-%m-%dT%H:%M:%S.%f)'], '%Y-%m-%dT%H:%M:%S.%f')\n",
        "    starttime = tr.stats.starttime.datetime\n",
        "    arrival = (arrival_time - starttime).total_seconds()\n",
        "\n",
        "    df = tr.stats.sampling_rate\n",
        "    sta_len = 180\n",
        "    lta_len = 1200\n",
        "    thr_on = 4\n",
        "    thr_off = 0.5\n",
        "\n",
        "    cft = classic_sta_lta(tr_data, int(sta_len * df), int(lta_len * df))\n",
        "    on_off = np.array(trigger_onset(cft, thr_on, thr_off))\n",
        "\n",
        "    df_triggers = create_trigger_dataframe(cft, nombre_archivo,arrival, on_off, tr_times)\n",
        "    return df_triggers\n",
        "\n",
        "\n",
        "def procesar_todos_los_archivos(muestra_cat, planeta):\n",
        "    \"\"\"\n",
        "    Processes all seismic files and extracts trigger information for each using the STA/LTA algorithm.\n",
        "    \"\"\"\n",
        "    todos_los_triggers = []\n",
        "    for nombre_archivo in tqdm(muestra_cat[\"filename\"], desc=\"Procesando archivos\"):\n",
        "        try:\n",
        "            df_triggers = obtencion_ventanas_sta_lta(nombre_archivo, planeta)\n",
        "            df_triggers['archivo_origen'] = nombre_archivo\n",
        "            todos_los_triggers.append(df_triggers)\n",
        "        except Exception as e:\n",
        "            print(f\"Error procesando {nombre_archivo}: {str(e)}\")\n",
        "\n",
        "    df_final = pd.concat(todos_los_triggers, ignore_index=True)\n",
        "    return df_final"
      ],
      "metadata": {
        "id": "069IqJ1GiXMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "planeta = \"luna\"\n",
        "df_final = procesar_todos_los_archivos(muestra_cat, planeta)"
      ],
      "metadata": {
        "id": "H3ZSXPyhiaR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ISOLATION FOREST\n",
        "To obtain the window or trigger that truly represents the seism, we calculate for each point in the series its anomaly coefficient, obtained with isolation forest.\n",
        "\n",
        "[Wank, P. (2019). Discover Unusual Patterns in Time Series Data with Unsupervised Anomaly Detection and Isolation Forest. *Medium*.](https://medium.com/@pw33392/discover-unusual-patterns-in-time-series-data-with-unsupervised-anomaly-detection-and-isolation-78db408caaed)"
      ],
      "metadata": {
        "id": "Pe2B2tYbic2t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class IsolationTree:\n",
        "    X: np.ndarray\n",
        "    indices: np.ndarray\n",
        "    max_depth: int\n",
        "    left: Optional['IsolationTree'] = None\n",
        "    right: Optional['IsolationTree'] = None\n",
        "    split_feature: Optional[int] = None\n",
        "    split_value: Optional[float] = None\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.X.shape[0] <= 1 or self.max_depth <= 0:\n",
        "            return\n",
        "\n",
        "\n",
        "        self.split_feature = np.random.randint(self.X.shape[1])\n",
        "\n",
        "        min_val = self.X[:, self.split_feature].min()\n",
        "        max_val = self.X[:, self.split_feature].max()\n",
        "        if min_val == max_val:\n",
        "            return\n",
        "        self.split_value = np.random.uniform(min_val, max_val)\n",
        "\n",
        "        left_indices = self.X[:, self.split_feature] < self.split_value\n",
        "        right_indices = self.X[:, self.split_feature] >= self.split_value\n",
        "\n",
        "        self.left = IsolationTree(self.X[left_indices], self.indices[left_indices], self.max_depth - 1)\n",
        "        self.right = IsolationTree(self.X[right_indices], self.indices[right_indices], self.max_depth - 1)\n",
        "\n",
        "    def path_lengths(self) -> Dict[int, int]:\n",
        "        if self.left is None and self.right is None:\n",
        "            return {idx: 1 for idx in self.indices}\n",
        "\n",
        "        path_lengths = {}\n",
        "        if self.left is not None:\n",
        "            left_path_lengths = self.left.path_lengths()\n",
        "            path_lengths.update({idx: length + 1 for idx, length in left_path_lengths.items()})\n",
        "        if self.right is not None:\n",
        "            right_path_lengths = self.right.path_lengths()\n",
        "            path_lengths.update({idx: length + 1 for idx, length in right_path_lengths.items()})\n",
        "        return path_lengths\n",
        "\n",
        "def isolation_forest_for_time_series(X: np.ndarray, window_size: int, n_trees: int, max_depth: int, sample_frac: float) -> np.ndarray:\n",
        "\n",
        "    windows = np.lib.stride_tricks.sliding_window_view(X, window_size)\n",
        "    num_windows = windows.shape[0]\n",
        "    indices = np.arange(num_windows)\n",
        "\n",
        "    path_lengths_sum = defaultdict(int)\n",
        "    path_lengths_counts = defaultdict(int)\n",
        "    for _ in range(n_trees):\n",
        "\n",
        "        sample_size = max(1, int(num_windows * sample_frac))\n",
        "        sample_indices = np.random.choice(num_windows, size=sample_size, replace=False)\n",
        "        sample_windows = windows[sample_indices]\n",
        "        tree = IsolationTree(sample_windows, sample_indices, max_depth)\n",
        "        path_lengths = tree.path_lengths()\n",
        "        for idx, path_length in path_lengths.items():\n",
        "            path_lengths_sum[idx] += path_length\n",
        "            path_lengths_counts[idx] += 1\n",
        "\n",
        "    avg_path_lengths = np.array([path_lengths_sum[idx] / path_lengths_counts[idx] for idx in range(num_windows)])\n",
        "    c = 2 * (np.log(num_windows - 1) + 0.5772156649) - (2 * (num_windows - 1) / num_windows)\n",
        "    anomaly_scores = 2 ** (-avg_path_lengths / c)\n",
        "    return anomaly_scores"
      ],
      "metadata": {
        "id": "P_PMvU-fiebl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_seismic_trace_by_filename(cat, filename, data_directory):\n",
        "    \"\"\"\n",
        "    Retrieves and plots the seismic trace data for a given filename from the specified directory.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        row = cat.loc[filename]\n",
        "    except KeyError:\n",
        "        print(f\"No se encontró el archivo {filename} en el catálogo.\")\n",
        "        return\n",
        "\n",
        "    if isinstance(row, pd.DataFrame):\n",
        "        row = row.iloc[0]\n",
        "\n",
        "    arrival_time_str = row['time_abs(%Y-%m-%dT%H:%M:%S.%f)']\n",
        "    arrival_time = datetime.strptime(arrival_time_str,'%Y-%m-%dT%H:%M:%S.%f')\n",
        "\n",
        "    mseed_file = f'{data_directory}{filename}.mseed'\n",
        "\n",
        "    try:\n",
        "        st = read(mseed_file)\n",
        "    except Exception as e:\n",
        "        print(f\"Error al leer el archivo {mseed_file}: {e}\")\n",
        "        return\n",
        "\n",
        "    tr = st[0].copy()\n",
        "    tr_times = tr.times()\n",
        "    tr_data = tr.data\n",
        "    starttime = tr.stats.starttime.datetime\n",
        "\n",
        "    arrival = (arrival_time - starttime).total_seconds()\n",
        "    return tr_times, tr_data"
      ],
      "metadata": {
        "id": "moq7DSJJilct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def obtencion_anomalias(planeta):\n",
        "    \"\"\"\n",
        "    Detects anomalies in seismic data for a given planet (Mars or Moon) using the Isolation Forest algorithm.\n",
        "    \"\"\"\n",
        "  if planeta.lower() == \"luna\":\n",
        "        data_directory = '/content/drive/My Drive/NASA/data/lunar/training/data/S12_GradeA/'\n",
        "  elif planeta.lower() == \"marte\":\n",
        "        data_directory = '/content/drive/My Drive/NASA/data/mars/training/data/'\n",
        "  else:\n",
        "        raise ValueError(\"El planeta debe ser 'luna' o 'marte'\")\n",
        "  todos_los_archivos = []\n",
        "  for filename in muestra_cat.index:\n",
        "    tr_times, tr_data = plot_seismic_trace_by_filename(muestra_cat, filename, data_directory)\n",
        "    df = pd.DataFrame({'time': tr_times, 'velocity': tr_data})\n",
        "    X = df['velocity'].values\n",
        "    window_size = 50\n",
        "    n_trees = 100\n",
        "    max_depth = 10\n",
        "    sample_frac = 0.5\n",
        "    anomaly_scores = isolation_forest_for_time_series(X, window_size, n_trees, max_depth, sample_frac)\n",
        "    scaled_anomaly_scores = MinMaxScaler().fit_transform(anomaly_scores.reshape(-1, 1)).flatten()\n",
        "    window_times = df['time'].iloc[window_size - 1:].reset_index(drop=True)\n",
        "    threshold = np.percentile(anomaly_scores, 95)\n",
        "    anomalies = anomaly_scores > threshold\n",
        "    anomaly_times = window_times[anomalies]\n",
        "    anomaly_df = pd.DataFrame({'time': anomaly_times, 'Filename': filename})\n",
        "    todos_los_archivos.append(anomaly_df)\n",
        "\n",
        "  anomaly_df = pd.concat(todos_los_archivos, ignore_index=True)\n",
        "  return anomaly_df"
      ],
      "metadata": {
        "id": "KR_6nzYziqcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anomaly_df = obtencion_anomalias(\"luna\")"
      ],
      "metadata": {
        "id": "Avp98vQ9ivgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def contar_anomalias(row, anomaly_df):\n",
        "    \"\"\"\n",
        "    Counts the number of anomalies within the specified time window for a given file.\n",
        "    \"\"\"\n",
        "    anomalias_archivo = anomaly_df[anomaly_df['Filename'] == row['Filename']]\n",
        "    # print(anomalias_archivo)\n",
        "    count = anomalias_archivo[(anomalias_archivo['time'] >= row['On Time']) &\n",
        "                              (anomalias_archivo['time'] <= row['Off Time'])].shape[0]\n",
        "\n",
        "    return count\n",
        "\n",
        "df_final['conteo_anomalias'] = df_final.apply(lambda row: contar_anomalias(row, anomaly_df), axis=1)\n",
        "df_seism = df_final.loc[df_final.groupby('Filename')['conteo_anomalias'].idxmax()]\n",
        "df_seism = df_seism.reset_index(drop=True)\n",
        "df_seism['Arrival_On_Diff'] = abs(df_seism['Arrival'] - df_seism['On Time'])"
      ],
      "metadata": {
        "id": "zat2phEvi1dg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_diff = df_seism['Arrival_On_Diff'].mean()\n",
        "print(f\"La media de la diferencia absoluta entre Arrival y On Time es: {mean_diff}\")\n",
        "\n",
        "plt.figure(figsize=(12, 7))\n",
        "plt.hist(df_seism['Arrival_On_Diff'], bins=100, edgecolor='black')\n",
        "plt.title('Histograma de la diferencia absoluta entre Arrival y valor predicho', fontsize=14)\n",
        "plt.xlabel('Diferencia absoluta', fontsize=12)\n",
        "plt.ylabel('Ficheros', fontsize=12)\n",
        "\n",
        "max_value = df_seism['Arrival_On_Diff'].max()\n",
        "x_ticks = range(0, int(max_value) + 1000, 1000)\n",
        "plt.xticks(x_ticks, rotation=45, fontsize=10)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VvZ9h2K9jK_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TEST DATA"
      ],
      "metadata": {
        "id": "jH_SuULs6QBI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_data_dir = '/content/drive/My Drive/NASA/test_data/'"
      ],
      "metadata": {
        "id": "ZNpHVkvX6UNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## STA/LTA for Test Data"
      ],
      "metadata": {
        "id": "kb4YrcG76rsb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cat_mars_directory_test = '/content/drive/My Drive/NASA/data/mars/test/data/'\n",
        "cat_lunar_directory_test = '/content/drive/My Drive/NASA/data/lunar/test/data/'\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "def copy_mseed_files(source_dirs, destination_dir):\n",
        "    \"\"\"\n",
        "    Copies all '.mseed' files from the specified source directories to the destination directory.\n",
        "    \"\"\"\n",
        "    Path(destination_dir).mkdir(parents=True, exist_ok=True)\n",
        "    mseed_files = []\n",
        "    for source_dir in source_dirs:\n",
        "        for root, dirs, files in os.walk(source_dir):\n",
        "            for file in files:\n",
        "                if file.endswith('.mseed'):\n",
        "                    full_path = os.path.join(root, file)\n",
        "                    mseed_files.append(full_path)\n",
        "\n",
        "    for file_path in mseed_files:\n",
        "        file_name = os.path.basename(file_path)\n",
        "        destination_path = os.path.join(destination_dir, file_name)\n",
        "        shutil.copy2(file_path, destination_path)\n",
        "        print(f\"Copiado: {file_path} -> {destination_path}\")\n",
        "\n",
        "    return len(mseed_files)\n",
        "\n",
        "# Directorios fuente\n",
        "cat_mars_directory_test = '/content/drive/My Drive/NASA/data/mars/test/data/'\n",
        "cat_lunar_directory_test = '/content/drive/My Drive/NASA/data/lunar/test/data/'\n",
        "\n",
        "\n",
        "source_dirs = [cat_mars_directory_test, cat_lunar_directory_test]\n",
        "total_files = copy_mseed_files(source_dirs, test_data_dir)\n",
        "print(f\"Total de archivos .mseed copiados: {total_files}\")"
      ],
      "metadata": {
        "id": "9uzsX-bC6XX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "muestra_cat_test = pd.DataFrame({'filename' : os.listdir(test_data_dir)})"
      ],
      "metadata": {
        "id": "pcf2zVvl6Zmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_trigger_dataframe(cft, nombre_archivo, on_off, tr_times):\n",
        "    \"\"\"\n",
        "    Create a dataframe containing detailed trigger information based on the CFT data.\n",
        "    \"\"\"\n",
        "    trigger_data = []\n",
        "    for i, (on, off) in enumerate(on_off):\n",
        "        cft_segment = cft[on:off+1]\n",
        "        trigger_data.append({\n",
        "            'Filename': nombre_archivo,\n",
        "            'Trigger': i + 1,\n",
        "            'On Time': tr_times[on],\n",
        "            'Off Time': tr_times[off],\n",
        "            'On Index': on,\n",
        "            'Off Index': off,\n",
        "            'Duration': tr_times[off] - tr_times[on],\n",
        "            'CFT On': cft[on],\n",
        "            'CFT Off': cft[off],\n",
        "            'CFT Max': np.max(cft_segment),\n",
        "            'CFT Min': np.min(cft_segment),\n",
        "            'CFT Mean': np.mean(cft_segment),\n",
        "            'CFT Median': np.median(cft_segment),\n",
        "            'CFT Std': np.std(cft_segment),\n",
        "            'CFT Skewness': stats.skew(cft_segment),\n",
        "            'CFT Kurtosis': stats.kurtosis(cft_segment),\n",
        "            'CFT Q1': np.percentile(cft_segment, 25),\n",
        "            'CFT Q3': np.percentile(cft_segment, 75),\n",
        "            'CFT IQR': np.percentile(cft_segment, 75) - np.percentile(cft_segment, 25),\n",
        "            'CFT Range': np.max(cft_segment) - np.min(cft_segment),\n",
        "            'CFT Coeff of Variation': np.std(cft_segment) / np.mean(cft_segment) if np.mean(cft_segment) != 0 else np.nan,\n",
        "            'CFT Peak to Mean Ratio': np.max(cft_segment) / np.mean(cft_segment) if np.mean(cft_segment) != 0 else np.nan,\n",
        "            'CFT Area Under Curve': np.trapz(cft_segment, tr_times[on:off+1]),\n",
        "            'CFT Max Index': on + np.argmax(cft_segment),\n",
        "            'CFT Max Time': tr_times[on + np.argmax(cft_segment)],\n",
        "            'CFT Slope': np.polyfit(tr_times[on:off+1], cft_segment, 1)[0]\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(trigger_data)\n",
        "    return df\n",
        "\n",
        "\n",
        "def obtencion_ventanas_sta_lta(nombre_archivo):\n",
        "    \"\"\"\n",
        "    Process a file to obtain the activation windows based on the STA/LTA algorithm.\n",
        "    \"\"\"\n",
        "    ruta_entrenamiento = test_data_dir\n",
        "\n",
        "    data = read(f\"{ruta_entrenamiento}{nombre_archivo}\")\n",
        "    tr = data.traces[0].copy()\n",
        "    tr_times = tr.times()\n",
        "    tr_data = tr.data\n",
        "\n",
        "    df = tr.stats.sampling_rate\n",
        "    sta_len = 180\n",
        "    lta_len = 1200\n",
        "    thr_on = 4\n",
        "    thr_off = 0.5\n",
        "\n",
        "    cft = classic_sta_lta(tr_data, int(sta_len * df), int(lta_len * df))\n",
        "    on_off = np.array(trigger_onset(cft, thr_on, thr_off))\n",
        "\n",
        "    df_triggers = create_trigger_dataframe(cft, nombre_archivo, on_off, tr_times)\n",
        "    return df_triggers\n",
        "\n",
        "\n",
        "def procesar_todos_los_archivos(muestra_cat_test):\n",
        "    \"\"\"\n",
        "    Process all files from the provided sample, obtaining trigger data for each file.\n",
        "    \"\"\"\n",
        "    todos_los_triggers = []\n",
        "    for nombre_archivo in tqdm(muestra_cat_test[\"filename\"], desc=\"Procesando archivos\"):\n",
        "        try:\n",
        "            df_triggers = obtencion_ventanas_sta_lta(nombre_archivo)\n",
        "            df_triggers['archivo_origen'] = nombre_archivo\n",
        "            todos_los_triggers.append(df_triggers)\n",
        "        except Exception as e:\n",
        "            print(f\"Error procesando {nombre_archivo}: {str(e)}\")\n",
        "\n",
        "    df_final = pd.concat(todos_los_triggers, ignore_index=True)\n",
        "    return df_final"
      ],
      "metadata": {
        "id": "OY7Oz6Em6bzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_final_test = procesar_todos_los_archivos(muestra_cat_test)"
      ],
      "metadata": {
        "id": "kglGhLjF6ew3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_final_test"
      ],
      "metadata": {
        "id": "dRyqZ0Bm6f_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Isolation Forest for Test Data"
      ],
      "metadata": {
        "id": "zd6BFAmR6h5-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class IsolationTree:\n",
        "    X: np.ndarray\n",
        "    indices: np.ndarray\n",
        "    max_depth: int\n",
        "    left: Optional['IsolationTree'] = None\n",
        "    right: Optional['IsolationTree'] = None\n",
        "    split_feature: Optional[int] = None\n",
        "    split_value: Optional[float] = None\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.X.shape[0] <= 1 or self.max_depth <= 0:\n",
        "            return\n",
        "\n",
        "\n",
        "        self.split_feature = np.random.randint(self.X.shape[1])\n",
        "\n",
        "        min_val = self.X[:, self.split_feature].min()\n",
        "        max_val = self.X[:, self.split_feature].max()\n",
        "        if min_val == max_val:\n",
        "            return\n",
        "        self.split_value = np.random.uniform(min_val, max_val)\n",
        "\n",
        "        left_indices = self.X[:, self.split_feature] < self.split_value\n",
        "        right_indices = self.X[:, self.split_feature] >= self.split_value\n",
        "\n",
        "        self.left = IsolationTree(self.X[left_indices], self.indices[left_indices], self.max_depth - 1)\n",
        "        self.right = IsolationTree(self.X[right_indices], self.indices[right_indices], self.max_depth - 1)\n",
        "\n",
        "    def path_lengths(self) -> Dict[int, int]:\n",
        "        if self.left is None and self.right is None:\n",
        "            return {idx: 1 for idx in self.indices}\n",
        "\n",
        "        path_lengths = {}\n",
        "        if self.left is not None:\n",
        "            left_path_lengths = self.left.path_lengths()\n",
        "            path_lengths.update({idx: length + 1 for idx, length in left_path_lengths.items()})\n",
        "        if self.right is not None:\n",
        "            right_path_lengths = self.right.path_lengths()\n",
        "            path_lengths.update({idx: length + 1 for idx, length in right_path_lengths.items()})\n",
        "        return path_lengths\n",
        "\n",
        "def isolation_forest_for_time_series(X: np.ndarray, window_size: int, n_trees: int, max_depth: int, sample_frac: float) -> np.ndarray:\n",
        "\n",
        "    windows = np.lib.stride_tricks.sliding_window_view(X, window_size)\n",
        "    num_windows = windows.shape[0]\n",
        "    indices = np.arange(num_windows)\n",
        "\n",
        "    path_lengths_sum = defaultdict(int)\n",
        "    path_lengths_counts = defaultdict(int)\n",
        "    for _ in range(n_trees):\n",
        "\n",
        "        sample_size = max(1, int(num_windows * sample_frac))\n",
        "        sample_indices = np.random.choice(num_windows, size=sample_size, replace=False)\n",
        "        sample_windows = windows[sample_indices]\n",
        "        tree = IsolationTree(sample_windows, sample_indices, max_depth)\n",
        "        path_lengths = tree.path_lengths()\n",
        "        for idx, path_length in path_lengths.items():\n",
        "            path_lengths_sum[idx] += path_length\n",
        "            path_lengths_counts[idx] += 1\n",
        "\n",
        "    avg_path_lengths = np.array([path_lengths_sum[idx] / path_lengths_counts[idx] for idx in range(num_windows)])\n",
        "    c = 2 * (np.log(num_windows - 1) + 0.5772156649) - (2 * (num_windows - 1) / num_windows)\n",
        "    anomaly_scores = 2 ** (-avg_path_lengths / c)\n",
        "    return anomaly_scores"
      ],
      "metadata": {
        "id": "adszXFVt6mZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_seismic_trace_by_filename(cat, filename, data_directory):\n",
        "    try:\n",
        "        row = cat.loc[filename]\n",
        "    except KeyError:\n",
        "        print(f\"No se encontró el archivo {filename} en el catálogo.\")\n",
        "        return\n",
        "\n",
        "    if isinstance(row, pd.DataFrame):\n",
        "        row = row.iloc[0]\n",
        "\n",
        "    mseed_file = f'{data_directory}{filename}'\n",
        "\n",
        "    try:\n",
        "        st = read(mseed_file)\n",
        "    except Exception as e:\n",
        "        print(f\"Error al leer el archivo {mseed_file}: {e}\")\n",
        "        return\n",
        "\n",
        "    tr = st[0].copy()\n",
        "    tr_times = tr.times()\n",
        "    tr_data = tr.data\n",
        "\n",
        "    return tr_times, tr_data"
      ],
      "metadata": {
        "id": "88rK7Aun60_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "muestra_cat_test.set_index('filename', inplace=True)"
      ],
      "metadata": {
        "id": "vUZlkO-062dw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def obtencion_anomalias():\n",
        "  data_directory = test_data_dir\n",
        "  todos_los_archivos = []\n",
        "  for filename in muestra_cat_test.index:\n",
        "    tr_times, tr_data = plot_seismic_trace_by_filename(muestra_cat_test, filename, data_directory)\n",
        "    df = pd.DataFrame({'time': tr_times, 'velocity': tr_data})\n",
        "    X = df['velocity'].values\n",
        "    window_size = 50\n",
        "    n_trees = 100\n",
        "    max_depth = 10\n",
        "    sample_frac = 0.5\n",
        "    anomaly_scores = isolation_forest_for_time_series(X, window_size, n_trees, max_depth, sample_frac)\n",
        "    scaled_anomaly_scores = MinMaxScaler().fit_transform(anomaly_scores.reshape(-1, 1)).flatten()\n",
        "    window_times = df['time'].iloc[window_size - 1:].reset_index(drop=True)\n",
        "    threshold = np.percentile(anomaly_scores, 95)\n",
        "    anomalies = anomaly_scores > threshold\n",
        "    anomaly_times = window_times[anomalies]\n",
        "    anomaly_df = pd.DataFrame({'time': anomaly_times, 'Filename': filename})\n",
        "    todos_los_archivos.append(anomaly_df)\n",
        "\n",
        "  anomaly_df = pd.concat(todos_los_archivos, ignore_index=True)\n",
        "  return anomaly_df"
      ],
      "metadata": {
        "id": "qcIMvtrj63xE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anomaly_df = obtencion_anomalias()"
      ],
      "metadata": {
        "id": "_2Ny4aLp66ka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We save the file containing the anomaly points identified from Isolation forest algorithm:"
      ],
      "metadata": {
        "id": "dg7Y4CAQ7Mn1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ruta_carpeta = '/content/drive/My Drive/NASA/'\n",
        "if not os.path.exists(ruta_carpeta):\n",
        "    os.makedirs(ruta_carpeta)\n",
        "ruta_archivo = os.path.join(ruta_carpeta, 'test_results.csv')\n",
        "\n",
        "anomaly_df.to_csv(ruta_archivo, index=False)"
      ],
      "metadata": {
        "id": "fNZf9Vz0673d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_final_test['conteo_anomalias'] = df_final_test.apply(lambda row: contar_anomalias(row, anomaly_df), axis=1)"
      ],
      "metadata": {
        "id": "IKN3Oy2F7Aku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_final_test.head()"
      ],
      "metadata": {
        "id": "5ynhcRu67EH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, the final file is generated, capturing the start time of the seismic event along with the file name. Seismic events were detected in 95 out of the 105 files."
      ],
      "metadata": {
        "id": "02k2806h9QeC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_seism_test = df_final_test.loc[df_final_test.groupby('Filename')['conteo_anomalias'].idxmax()]\n",
        "df_seism_test = df_seism_test.reset_index(drop=True)\n",
        "predicciones = df_seism_test[['Filename','On Time']]\n",
        "print(predicciones)"
      ],
      "metadata": {
        "id": "z7B3vFX27Fn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicciones.to_csv('/content/drive/My Drive/NASA/test_predictions_csv.csv', index=False)"
      ],
      "metadata": {
        "id": "kNV5lWkf7HXB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}